---
title: Pytorch Lightning ã®ä½¿ã„æ–¹

date: 2022-10-18 12:00:00
categories: [AI]
tags: [Deep Learning, PyTorch, Python, æ©Ÿæ¢°å­¦ç¿’, AI, äººå·¥çŸ¥èƒ½, æ·±å±¤å­¦ç¿’]
lang: ja

description:
---

## ç›®æ¬¡

---

## PyTorch Lightning

PyTorch Lightning ã¯ã€PyTorch ã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹è»½é‡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ä»¥ä¸‹ã«åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ã¾ãš PyTorch Lightning ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™:

```bash
pip install lightning
```

```conda
conda install lightning -c conda-forge
```

## ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©

`LightningModule` ã‚’ç¶™æ‰¿ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚ä»¥ä¸‹ã¯ç°¡å˜ãªä¾‹ã§ã™:

```python
import pytorch_lightning as pl

# LightningModule ã‚’ç¶™æ‰¿ã—ãŸã‚¯ãƒ©ã‚¹å®šç¾©
class LightningModule(L.LightningModule):
    def __init__(self, model, criterion, lr):
        super().__init__()
        self.model = model      # ãƒ¢ãƒ‡ãƒ«
        self.lr = lr            # å­¦ç¿’ç‡
        self.criterion = criterion  # æå¤±é–¢æ•°

    # å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã®å®šç¾©
    def training_step(self, batch, batch_idx):
        inputs, labels = batch
        outputs = self.model(inputs)
        loss = self.criterion(outputs, labels)
        self.log("train_loss", loss, prog_bar=True)  # å­¦ç¿’æå¤±ã®è¨˜éŒ²
        return loss

    # æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ã®å®šç¾©
    def validation_step(self, batch, batch_idx):
        inputs, labels = batch
        outputs, _ = self.model(inputs)  # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬
        loss = self.criterion(outputs, labels)
        # æ­£è§£ç‡ï¼ˆaccuracyï¼‰ã‚’è¨ˆç®—
        acc = torch.sum(torch.argmax(outputs, dim=1) == labels).item() / len(labels)
        self.log("val_loss", loss, prog_bar=True)  # æ¤œè¨¼æå¤±ã®è¨˜éŒ²
        self.log("val_acc", acc, prog_bar=True)    # æ¤œè¨¼ç²¾åº¦ã®è¨˜éŒ²
        return loss

    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®å®šç¾©
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            self.lr,
            betas=(0.9, 0.999),
        )
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=-1,
            gamma=0.1,
        )
        return [optimizer], [scheduler]
```

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™

### pytorch ã® dataloader ã‚’ä½¿ã†å ´åˆ

```python
# ## build dataset
train_dataset = ImageFolder(
    root=data_root / train_data_path,
    transform=train_transformer,
)
val_dataset = ImageFolder(
    root=data_root / val_data_path,
    transform=val_transformer,
)

# ## build dataloader
train_dataloader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    shuffle=True,
)
val_dataloader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    shuffle=False,
)
```

### LightningDataModule ã‚’ä½¿ã†å ´åˆ

```python
from pytorch_lightning import LightningDataModule
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from torchvision.datasets import MNIST

class MyDataModule(LightningDataModule):
    def __init__(self, batch_size=32):
        super().__init__()
        self.batch_size = batch_size
        self.transform = transforms.ToTensor()

    def prepare_data(self):
        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãªã©ã€1å›ã ã‘å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†
        MNIST(root='./data', train=True, download=True)
        MNIST(root='./data', train=False, download=True)

    def setup(self, stage=None):
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²ã‚„å‰å‡¦ç†ã‚’å®šç¾©
        full_dataset = MNIST(root='./data', train=True, transform=self.transform)
        self.train_dataset, self.val_dataset = random_split(full_dataset, [55000, 5000])
        self.test_dataset = MNIST(root='./data', train=False, transform=self.transform)

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size)

    def test_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.batch_size)
```

#### ğŸ” å„ãƒ¡ã‚½ãƒƒãƒ‰ã®å½¹å‰²

| ãƒ¡ã‚½ãƒƒãƒ‰å           | èª¬æ˜                                                                               |
| -------------------- | ---------------------------------------------------------------------------------- |
| `prepare_data()`     | ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãªã©ã€1 åº¦ã ã‘å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†ï¼ˆãƒãƒ«ãƒ GPU ç’°å¢ƒã§ã‚‚ 1 å›ã®ã¿ï¼‰   |
| `setup()`            | å­¦ç¿’/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆãƒ»åˆ†å‰²ã‚’è¡Œã†ï¼ˆåˆ†æ•£ç’°å¢ƒã§ã¯å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã§å‘¼ã°ã‚Œã‚‹ï¼‰ |
| `train_dataloader()` | å­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’è¿”ã™                                                       |
| `val_dataloader()`   | æ¤œè¨¼ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’è¿”ã™                                                       |
| `test_dataloader()`  | ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’è¿”ã™                                                     |
