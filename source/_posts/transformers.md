---
title: Transformersã€€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
date: 2024-3-30 17:10:00
categories: [AI]
tags: [Deep Learning, transformers, æ©Ÿæ¢°å­¦ç¿’, AI, äººå·¥çŸ¥èƒ½, æ·±å±¤å­¦ç¿’]
lang: ja
description: Transformers ã¯ã€PyTorch, TensorFlow, JAX ã«å¯¾å¿œã—ãŸæ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€æœ€å…ˆç«¯ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã‚„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€éŸ³å£°èªè­˜ãªã©ã•ã¾ã–ã¾ãªåˆ†é‡ã§ã®ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€æŸ”è»Ÿãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ç›¸äº’é‹ç”¨æ€§ã¨æœ¬ç•ªç’°å¢ƒå‘ã‘ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ©Ÿèƒ½ï¼ˆONNX ã‚„ TorchScript å½¢å¼ã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼‰ã‚’æä¾›ã—ã¾ã™ã€‚
---

## ç›®æ¬¡

- [Transformers ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯](#transformers-%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%83%AF%E3%83%BC%E3%82%AF)
  - [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB)
    - [pipã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#pip%E3%81%A7%E3%81%AE%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB)
    - [ç·¨é›†å¯èƒ½ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#%E7%B7%A8%E9%9B%86%E5%8F%AF%E8%83%BD%E3%81%AA%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB)
    - [ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š](#%E3%82%AD%E3%83%A3%E3%83%83%E3%82%B7%E3%83%A5%E3%81%AE%E8%A8%AD%E5%AE%9A)
    - [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰](#%E3%82%AA%E3%83%95%E3%83%A9%E3%82%A4%E3%83%B3%E3%83%A2%E3%83%BC%E3%83%89)
    - [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ã®ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆ©ç”¨æ–¹æ³•](#%E3%82%AA%E3%83%95%E3%83%A9%E3%82%A4%E3%83%B3%E3%81%A7%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%83%88%E3%83%BC%E3%82%AF%E3%83%8A%E3%82%A4%E3%82%B6%E3%83%BC%E5%88%A9%E7%94%A8%E6%96%B9%E6%B3%95)
      - [**Model Hub UI** ã‹ã‚‰æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](#model-hub-ui-%E3%81%8B%E3%82%89%E6%89%8B%E5%8B%95%E3%81%A7%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89)
      - [PreTrainedModel.from\_pretrained() \& save\_pretrained() ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](#pretrainedmodelfrompretrained--savepretrained-%E3%83%AF%E3%83%BC%E3%82%AF%E3%83%95%E3%83%AD%E3%83%BC)
      - [huggingface\_hubãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ãªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](#huggingfacehub%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%9F%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0%E7%9A%84%E3%81%AA%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89)
  - [ä½¿ã„æ–¹](#%E4%BD%BF%E3%81%84%E6%96%B9)
    - [ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¦‚è¦](#%E4%B8%BB%E8%A6%81%E3%82%B3%E3%83%B3%E3%83%9D%E3%83%BC%E3%83%8D%E3%83%B3%E3%83%88%E6%A6%82%E8%A6%81)
      - [ç°¡å˜ãªæ¨è«–ã‚³ãƒ¼ãƒ‰ä¾‹:](#%E7%B0%A1%E5%8D%98%E3%81%AA%E6%8E%A8%E8%AB%96%E3%82%B3%E3%83%BC%E3%83%89%E4%BE%8B)
    - [Pipeline](#pipeline)
      - [ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰:](#%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89)
    - [Tokenizers](#tokenizers)
      - [åˆ†è©å™¨ã®ä¸»ãªæ©Ÿèƒ½](#%E5%88%86%E8%A9%9E%E5%99%A8%E3%81%AE%E4%B8%BB%E3%81%AA%E6%A9%9F%E8%83%BD)
        - [**Tokenize**](#tokenize)
        - [**Encode**](#encode)
        - [**Encode + ç‰¹æ®Šã‚¿ã‚°ä»˜åŠ **](#encode--%E7%89%B9%E6%AE%8A%E3%82%BF%E3%82%B0%E4%BB%98%E5%8A%A0)
        - [**Decode**](#decode)
      - [åˆ†è©å™¨ã®é«˜éšæ©Ÿèƒ½](#%E5%88%86%E8%A9%9E%E5%99%A8%E3%81%AE%E9%AB%98%E9%9A%8E%E6%A9%9F%E8%83%BD)
        - [ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨èªå½™æƒ…å ±](#%E7%89%B9%E6%AE%8A%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E3%81%A8%E8%AA%9E%E5%BD%99%E6%83%85%E5%A0%B1)
        - [ãƒãƒƒãƒå‡¦ç†ã¨é•·æ–‡å¯¾å¿œ](#%E3%83%90%E3%83%83%E3%83%81%E5%87%A6%E7%90%86%E3%81%A8%E9%95%B7%E6%96%87%E5%AF%BE%E5%BF%9C)
      - [å®Ÿè£…ä¾‹](#%E5%AE%9F%E8%A3%85%E4%BE%8B)
    - [å®Ÿç”¨çš„ãªè¨­å®šã¨æœ€é©åŒ–æŠ€æ³•](#%E5%AE%9F%E7%94%A8%E7%9A%84%E3%81%AA%E8%A8%AD%E5%AE%9A%E3%81%A8%E6%9C%80%E9%81%A9%E5%8C%96%E6%8A%80%E6%B3%95)
      - [ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚ã®æœ€é©åŒ–](#%E3%83%A2%E3%83%87%E3%83%AB%E3%83%AD%E3%83%BC%E3%83%89%E6%99%82%E3%81%AE%E6%9C%80%E9%81%A9%E5%8C%96)
      - [ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–](#%E3%83%90%E3%83%83%E3%83%81%E5%87%A6%E7%90%86%E3%81%AE%E6%9C%80%E9%81%A9%E5%8C%96)
    - [å‚è€ƒ](#%E5%8F%82%E8%80%83)


---

# Transformers ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

Transformers ã¯ã€PyTorch, TensorFlow, JAX ã«å¯¾å¿œã—ãŸæ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€æœ€å…ˆç«¯ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã‚„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€éŸ³å£°èªè­˜ãªã©ã•ã¾ã–ã¾ãªåˆ†é‡ã§ã®ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€æŸ”è»Ÿãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ç›¸äº’é‹ç”¨æ€§ã¨æœ¬ç•ªç’°å¢ƒå‘ã‘ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ©Ÿèƒ½ï¼ˆONNX ã‚„ TorchScript å½¢å¼ã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼‰ã‚’æä¾›ã—ã¾ã™ã€‚


## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
### pipã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

- å‰æã¨ã—ã¦ã€Pytorchã¾ãŸã¯tensorflowã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€ã•ã‚‰ã«ã€flaxã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚


ã“ã‚Œã§ã€æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§ğŸ¤— Transformersã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸ:

```
pip install transformers
```

CPUå¯¾å¿œã®ã¿å¿…è¦ãªå ´åˆã€ğŸ¤— Transformersã¨Deep Learningãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’1è¡Œã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¦ä¾¿åˆ©ã§ã™ã€‚ä¾‹ãˆã°ã€ğŸ¤— Transformersã¨PyTorchã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ä¸€ç·’ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™:

```
pip install transformers[torch]
```

ğŸ¤— Transformersã¨TensorFlow 2.0:

```
pip install transformers[tf-cpu]
```

ğŸ¤— Transformersã¨Flax:

```
pip install transformers[flax]
```

æœ€å¾Œã«ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ğŸ¤— TransformersãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™:

```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

ãã®å¾Œã€ãƒ©ãƒ™ãƒ«ã¨ã‚¹ã‚³ã‚¢ãŒå‡ºåŠ›ã•ã‚Œã¾ã™:

```
[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
```

### ç·¨é›†å¯èƒ½ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

å¿…è¦ã«å¿œã˜ã¦ã€ç·¨é›†å¯èƒ½ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã—ã¾ã™:

- ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®`main`ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ã„ã¾ã™ã€‚
- ğŸ¤— Transformersã«ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆã—ã€ã‚³ãƒ¼ãƒ‰ã®å¤‰æ›´ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ¬ãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¦ã€ğŸ¤— Transformersã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™:

```
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š
- å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ `~/.cache/huggingface/hub` ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚
- Windowsã§ã¯ã€`C:\Users\username\.cache\huggingface\hub` ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã™ã€‚
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã§å¤‰æ›´å¯èƒ½ï¼ˆå„ªå…ˆé †ä½é †ï¼‰:
  1. `HF_HUB_CACHE` or `TRANSFORMERS_CACHE`
  2. `HF_HOME`
  3. `XDG_CACHE_HOME` + `/huggingface`

> âš ï¸ éå»ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ `PYTORCH_TRANSFORMERS_CACHE` ã¾ãŸã¯ `PYTORCH_PRETRAINED_BERT_CACHE` ã‚’ä½¿ç”¨ã—ã¦ã„ãŸå ´åˆã€ãã‚Œã‚‰ãŒå¼•ãç¶šãä½¿ç”¨ã•ã‚Œã¾ã™ï¼ˆæ˜ç¤ºçš„ã« `TRANSFORMERS_CACHE` ã‚’è¨­å®šã—ã¦ã„ãªã„é™ã‚Šï¼‰ã€‚


### ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ã‚‚å‹•ä½œã•ã›ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™:
  - `HF_HUB_OFFLINE=1`: Hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãªã„
  - `HF_DATASETS_OFFLINE=1`: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¯¾å¿œ

ä¾‹:
```bash
HF_DATASETS_OFFLINE=1 HF_HUB_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...
```


### ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ã®ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆ©ç”¨æ–¹æ³•
#### **Model Hub UI** ã‹ã‚‰æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä¸Šã® â†“ ã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã€‚

#### PreTrainedModel.from_pretrained() & save_pretrained() ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
å‰ã‚‚ã£ã¦ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†ä¿å­˜:
```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")

tokenizer.save_pretrained("./your/path/bigscience_t0")
model.save_pretrained("./your/path/bigscience_t0")
```

ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§å†èª­ã¿è¾¼ã¿:
```python
tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
model = AutoModel.from_pretrained("./your/path/bigscience_t0")
```

#### huggingface_hubãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ãªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
```bash
python -m pip install huggingface_hub
```

ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:
```python
from huggingface_hub import hf_hub_download

hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
```

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã«ãƒ­ãƒ¼ãƒ‰:
```python
from transformers import AutoConfig

config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

## ä½¿ã„æ–¹

### ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¦‚è¦

HuggingFace Transformers ã¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€ä»¥ä¸‹ã®ã‚ˆã†ãªä¸»è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™:

- **`AutoTokenizer`**: ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†è©ã‚„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
- **`AutoModel`**: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®åŸºæœ¬ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
- **`Trainer`, `TrainingArguments`**: ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ãŸã‚ã®é«˜ãƒ¬ãƒ™ãƒ«ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
- **`Pipeline`**: å‰å‡¦ç†ã‹ã‚‰æ¨è«–ã€å¾Œå‡¦ç†ã¾ã§ã®å…¨ãƒ•ãƒ­ãƒ¼ã‚’ã‚«ãƒ—ã‚»ãƒ«åŒ–ã—ã¦ãŠã‚Šã€ç´ æ—©ãé–‹ç™ºã‚’é–‹å§‹ã§ãã¾ã™ã€‚

#### ç°¡å˜ãªæ¨è«–ã‚³ãƒ¼ãƒ‰ä¾‹:
```python
from transformers import AutoTokenizer, AutoModel

def basic_usage_example():
    # Tokenizerã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')
    model = AutoModel.from_pretrained('bert-base-chinese')
    
    # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
    text = "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œï¼"
    inputs = tokenizer(text, return_tensors="pt")  # PyTorchãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã™
    
    # æ¨è«–å®Ÿè¡Œ
    outputs = model(**inputs)
    
    # éš ã‚Œå±¤ã®æœ€çµ‚å‡ºåŠ›ã‚’è¿”ã™
    return outputs.last_hidden_state
```

### Pipeline

`pipeline` ã¯éå¸¸ã«ç°¡å˜ã«ã•ã¾ã–ã¾ãªNLPã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã™ã€‚

#### ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰:
```python
from transformers import pipeline

def pipeline_examples():
    """ä»£è¡¨çš„ãªã‚¿ã‚¹ã‚¯ã®Pipelineä¾‹"""

    # æ„Ÿæƒ…åˆ†æ
    sentiment_analyzer = pipeline("sentiment-analysis")
    result = sentiment_analyzer("ã“ã®è£½å“ã¯ã¨ã¦ã‚‚ä½¿ã„ã‚„ã™ã„ï¼")
    print(f"æ„Ÿæƒ…åˆ†æçµæœï¼š{result}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆGPT-2ï¼‰
    generator = pipeline("text-generation", model="gpt2-chinese")
    text = generator("äººå·¥çŸ¥èƒ½ã¯ä»Š", max_length=50)
    print(f"ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼š{text}")
    
    # å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰
    ner = pipeline("ner", model="bert-base-chinese")
    entities = ner("è¯ç‚ºã®æœ¬ç¤¾ã¯æ·±åœ³ã«ã‚ã‚Šã¾ã™")
    print(f"èªè­˜ã•ã‚ŒãŸå›ºæœ‰åè©ï¼š{entities}")
    
    # è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ 
    qa = pipeline("question-answering", model="bert-base-chinese")
    context = "åŒ—äº¬ã¯ä¸­å›½ã®é¦–éƒ½ã§ã‚ã‚Šã€ä¸Šæµ·ã¯æœ€å¤§ã®çµŒæ¸ˆéƒ½å¸‚ã§ã™ã€‚"
    question = "ä¸­å›½ã®é¦–éƒ½æ˜¯ã¯ã©ã“ã§ã™ã‹ï¼Ÿ"
    answer = qa(question=question, context=context)
    print(f"è³ªå•å¿œç­”çµæœï¼š{answer}")

if __name__ == "__main__":
    pipeline_examples()
```

### Tokenizers

Tokenizers ã¯è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã«ãŠã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹å½¢å¼ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®åŸºæœ¬çš„ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã™ã€‚  
HuggingFace Transformers ã§ã¯ã€å¤šæ§˜ãªè¨€èªã‚„ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã—ãŸæŸ”è»Ÿã§é«˜æ€§èƒ½ãª `Tokenizer API` ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

#### åˆ†è©å™¨ã®ä¸»ãªæ©Ÿèƒ½

##### **Tokenize**
- ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã€ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã€æ–‡å­—ãªã©ã®ã€Œãƒˆãƒ¼ã‚¯ãƒ³ã€ã«åˆ†å‰²ã—ã¾ã™ã€‚
- ä¾‹: `"ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆ"` â†’ `["ã“ã‚Œ", "ã¯", "ãƒ†ã‚¹ãƒˆ"]`

```python
tokens = tokenizer.tokenize("ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆ")
```

##### **Encode**
- ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•°å€¤ IDï¼ˆèªå½™IDï¼‰ã«å¤‰æ›ã—ã¾ã™ã€‚
- ä¾‹: `["ã“ã‚Œ", "ã¯", "ãƒ†ã‚¹ãƒˆ"]` â†’ `[345, 890, 1234]`

```python
token_ids = tokenizer.convert_tokens_to_ids(tokens)
```

##### **Encode + ç‰¹æ®Šã‚¿ã‚°ä»˜åŠ **
- å…¥åŠ›ã«ç‰¹æ®Šã‚¿ã‚°ï¼ˆ[CLS], [SEP]ãªã©ï¼‰ã‚’è¿½åŠ ã—ã€ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã§å‡ºåŠ›ã—ã¾ã™ã€‚

```python
encoded = tokenizer(text, return_tensors="pt")
```

##### **Decode**
- æ•°å€¤ ID ã‚’å†ã³äººé–“å¯èª­ãªãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã—ã¾ã™ã€‚

```python
decoded_text = tokenizer.decode(encoded["input_ids"][0])
```

#### åˆ†è©å™¨ã®é«˜éšæ©Ÿèƒ½

#####  ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨èªå½™æƒ…å ±

- **[CLS]**: åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨ã®ç‰¹åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³
- **[SEP]**: æ–‡ã®åŒºåˆ‡ã‚Šã‚’ç¤ºã™ãƒˆãƒ¼ã‚¯ãƒ³

```python
print(f"CLSæ ‡è®°: {tokenizer.cls_token}")      # [CLS]
print(f"SEPæ ‡è®°: {tokenizer.sep_token}")      # [SEP]
print(f"è¯è¡¨å¤§å°: {len(tokenizer)}")          # èªå½™æ•°
print(f"ç‰¹æ®Šæ ‡è®°æ˜ å°„: {tokenizer.special_tokens_map}")
```

##### ãƒãƒƒãƒå‡¦ç†ã¨é•·æ–‡å¯¾å¿œ

è¤‡æ•°æ–‡ã‚’ä¸€åº¦ã«å‡¦ç†ã—ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¨åˆ‡ã‚Šè©°ã‚ã‚‚è‡ªå‹•åŒ–ã§ãã¾ã™ã€‚

```python
batch_encoding = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
```

é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æœ€å¤§é•·ã§è‡ªå‹•çš„ã«åˆ‡ã‚Šè©°ã‚ã¦å‡¦ç†ã—ã¾ã™ã€‚

```python
truncated = tokenizer(long_text, max_length=128, truncation=True)
```

#### å®Ÿè£…ä¾‹

```python
from transformers import AutoTokenizer

def tokenizer_basics():
    # åˆ†è©å™¨ãƒ­ãƒ¼ãƒ‰
    tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
    text = "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆ"

    # 1. åˆ†è©
    tokens = tokenizer.tokenize(text)
    print(f"Result: {tokens}")

    # 2. Token IDs ã¸å¤‰æ›
    token_ids = tokenizer.convert_tokens_to_ids(tokens)
    print(f"Token IDs: {token_ids}")

    # 3. ç·¨ç¢¼ï¼ˆç‰¹æ®Šã‚¿ã‚°å«ã‚€ï¼‰
    encoded = tokenizer(text, return_tensors="pt")
    print(f"Encoding result: {encoded}")

    # 4. è§£ç 
    decoded = tokenizer.decode(encoded["input_ids"][0])
    print(f"Decoding result: {decoded}")

# ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±
def tokenizer_special_tokens():
    tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
    print(f"CLS map: {tokenizer.cls_token}")
    print(f"SEP map: {tokenizer.sep_token}")
    print(f"tokenizer length: {len(tokenizer)}")
    print(f"spcial token map: {tokenizer.special_tokens_map}")

# ãƒãƒƒãƒãƒ»é•·æ–‡å‡¦ç†
def batch_and_long_text_processing():
    tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
    texts = ["this is the first text", "this is the second text"]
    batch_encoding = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    print(f"batch processing result: {batch_encoding}")

    long_text = "this is a " + "very very very very very very very very very very " * 50 + "long textã€‚"""
    truncated = tokenizer(long_text, max_length=128, truncation=True)
    print(f"Result: {truncated['input_ids']}")

# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    tokenizer_basics()
    tokenizer_special_tokens()
    batch_and_long_text_processing()
```


### å®Ÿç”¨çš„ãªè¨­å®šã¨æœ€é©åŒ–æŠ€æ³•

#### ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚ã®æœ€é©åŒ–
ä»¥ä¸‹ã®æ–¹æ³•ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨æ¨è«–é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

```python
from transformers import AutoModel
import torch

def setup_optimization():
    """ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–è¨­å®š"""
    model = AutoModel.from_pretrained(
        "bert-base-chinese",
        device_map="auto",         # è‡ªå‹•çš„ã«GPU/CPUã«å‰²ã‚Šå½“ã¦
        torch_dtype=torch.float16, # åŠç²¾åº¦ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
        low_cpu_mem_usage=True     # CPUãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’æŠ‘ãˆã‚‹
    )
    model.eval()  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã¸åˆ‡ã‚Šæ›¿ãˆ
    return model
```

#### ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–
å¤§è¦æ¨¡ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†éš›ã«ã¯ã€ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Šå‡¦ç†é€Ÿåº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚é•·æ–‡ã®åˆ†å‰²ã‚‚è‡ªå‹•åŒ–ã—ã¦ã„ã¾ã™ã€‚

```python
from typing import List

def batch_process(texts: List[str], batch_size: int, max_length: int) -> List[List[str]]:
    """é•·æ–‡ã‚’åˆ†å‰²ã—ã€æŒ‡å®šãƒãƒƒãƒã‚µã‚¤ã‚ºã§å‡¦ç†ã™ã‚‹"""
    processed_texts = []
    for text in texts:
        if len(text) > max_length:
            chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]
            processed_texts.extend(chunks)
        else:
            processed_texts.append(text)
    
    return [processed_texts[i:i+batch_size] for i in range(0, len(processed_texts), batch_size)]
```



### å‚è€ƒ
- Hubã‹ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è©³ç´°ã«ã¤ã„ã¦ã¯ [Transformers official doc](https://huggingface.co/docs/transformers) ã‚’å‚ç…§ã€‚
- [Hugging Face Tokenizers Documentation](https://huggingface.co/docs/transformers/tokenizer_summary)