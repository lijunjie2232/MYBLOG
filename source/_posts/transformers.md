---
title: Transformersã€€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
date: 2024-3-30 17:10:00
categories: [AI]
tags: [Deep Learning, transformers, æ©Ÿæ¢°å­¦ç¿’, AI, äººå·¥çŸ¥èƒ½, æ·±å±¤å­¦ç¿’]
lang: ja
description: Transformers ã¯ã€PyTorch, TensorFlow, JAX ã«å¯¾å¿œã—ãŸæ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€æœ€å…ˆç«¯ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã‚„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€éŸ³å£°èªè­˜ãªã©ã•ã¾ã–ã¾ãªåˆ†é‡ã§ã®ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€æŸ”è»Ÿãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ç›¸äº’é‹ç”¨æ€§ã¨æœ¬ç•ªç’°å¢ƒå‘ã‘ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ©Ÿèƒ½ï¼ˆONNX ã‚„ TorchScript å½¢å¼ã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼‰ã‚’æä¾›ã—ã¾ã™ã€‚
---

## ç›®æ¬¡

- [Transformers ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯](#transformers-%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%83%AF%E3%83%BC%E3%82%AF)
  - [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB)
    - [pipã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#pip%E3%81%A7%E3%81%AE%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB)
    - [ç·¨é›†å¯èƒ½ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#%E7%B7%A8%E9%9B%86%E5%8F%AF%E8%83%BD%E3%81%AA%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB)
    - [ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š](#%E3%82%AD%E3%83%A3%E3%83%83%E3%82%B7%E3%83%A5%E3%81%AE%E8%A8%AD%E5%AE%9A)
    - [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰](#%E3%82%AA%E3%83%95%E3%83%A9%E3%82%A4%E3%83%B3%E3%83%A2%E3%83%BC%E3%83%89)
    - [ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ã®ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆ©ç”¨æ–¹æ³•](#%E3%82%AA%E3%83%95%E3%83%A9%E3%82%A4%E3%83%B3%E3%81%A7%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%83%88%E3%83%BC%E3%82%AF%E3%83%8A%E3%82%A4%E3%82%B6%E3%83%BC%E5%88%A9%E7%94%A8%E6%96%B9%E6%B3%95)
      - [**Model Hub UI** ã‹ã‚‰æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](#model-hub-ui-%E3%81%8B%E3%82%89%E6%89%8B%E5%8B%95%E3%81%A7%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89)
      - [PreTrainedModel.from\_pretrained() \& save\_pretrained() ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](#pretrainedmodelfrompretrained--savepretrained-%E3%83%AF%E3%83%BC%E3%82%AF%E3%83%95%E3%83%AD%E3%83%BC)
      - [huggingface\_hubãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ãªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](#huggingfacehub%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%9F%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0%E7%9A%84%E3%81%AA%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89)
  - [ä½¿ã„æ–¹](#%E4%BD%BF%E3%81%84%E6%96%B9)
    - [ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¦‚è¦](#%E4%B8%BB%E8%A6%81%E3%82%B3%E3%83%B3%E3%83%9D%E3%83%BC%E3%83%8D%E3%83%B3%E3%83%88%E6%A6%82%E8%A6%81)
      - [ç°¡å˜ãªæ¨è«–ã‚³ãƒ¼ãƒ‰ä¾‹:](#%E7%B0%A1%E5%8D%98%E3%81%AA%E6%8E%A8%E8%AB%96%E3%82%B3%E3%83%BC%E3%83%89%E4%BE%8B)
    - [Pipeline](#pipeline)
      - [ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰:](#%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89)
    - [Tokenizers](#tokenizers)
      - [åˆ†è©å™¨ã®ä¸»ãªæ©Ÿèƒ½](#%E5%88%86%E8%A9%9E%E5%99%A8%E3%81%AE%E4%B8%BB%E3%81%AA%E6%A9%9F%E8%83%BD)
        - [**Tokenize**](#tokenize)
        - [**Encode**](#encode)
        - [**Encode + ç‰¹æ®Šã‚¿ã‚°ä»˜åŠ **](#encode--%E7%89%B9%E6%AE%8A%E3%82%BF%E3%82%B0%E4%BB%98%E5%8A%A0)
        - [**Decode**](#decode)
      - [åˆ†è©å™¨ã®é«˜éšæ©Ÿèƒ½](#%E5%88%86%E8%A9%9E%E5%99%A8%E3%81%AE%E9%AB%98%E9%9A%8E%E6%A9%9F%E8%83%BD)
        - [ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨èªå½™æƒ…å ±](#%E7%89%B9%E6%AE%8A%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E3%81%A8%E8%AA%9E%E5%BD%99%E6%83%85%E5%A0%B1)
        - [ãƒãƒƒãƒå‡¦ç†ã¨é•·æ–‡å¯¾å¿œ](#%E3%83%90%E3%83%83%E3%83%81%E5%87%A6%E7%90%86%E3%81%A8%E9%95%B7%E6%96%87%E5%AF%BE%E5%BF%9C)
      - [å®Ÿè£…ä¾‹](#%E5%AE%9F%E8%A3%85%E4%BE%8B)
    - [Models](#models)
      - [æ–‡æ›¸åˆ†é¡ï¼ˆSequence Classificationï¼‰](#%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9Esequence-classification)
      - [è³ªå•å¿œç­”ï¼ˆQuestion Answeringï¼‰](#%E8%B3%AA%E5%95%8F%E5%BF%9C%E7%AD%94question-answering)
      - [ä¸»ãªãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ä¸€è¦§](#%E4%B8%BB%E3%81%AA%E3%83%A2%E3%83%87%E3%83%AB%E3%82%AF%E3%83%A9%E3%82%B9%E4%B8%80%E8%A6%A7)
      - [å®Ÿè£…ãƒ•ãƒ­ãƒ¼ã¾ã¨ã‚](#%E5%AE%9F%E8%A3%85%E3%83%95%E3%83%AD%E3%83%BC%E3%81%BE%E3%81%A8%E3%82%81)
    - [Configuration](#configuration)
      - [åŸºæœ¬è¨­å®šã®ãƒ­ãƒ¼ãƒ‰ã¨ç¢ºèª](#%E5%9F%BA%E6%9C%AC%E8%A8%AD%E5%AE%9A%E3%81%AE%E3%83%AD%E3%83%BC%E3%83%89%E3%81%A8%E7%A2%BA%E8%AA%8D)
      - [ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã®ä½œæˆ](#%E3%82%AB%E3%82%B9%E3%82%BF%E3%83%A0%E8%A8%AD%E5%AE%9A%E3%81%AE%E4%BD%9C%E6%88%90)
      - [è¨­å®šã®ä¿å­˜ã¨å†èª­ã¿è¾¼ã¿](#%E8%A8%AD%E5%AE%9A%E3%81%AE%E4%BF%9D%E5%AD%98%E3%81%A8%E5%86%8D%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF)
      - [ä¸»ãªè¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸€è¦§](#%E4%B8%BB%E3%81%AA%E8%A8%AD%E5%AE%9A%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E4%B8%80%E8%A6%A7)
    - [å®Ÿç”¨çš„ãªè¨­å®šã¨æœ€é©åŒ–æŠ€æ³•](#%E5%AE%9F%E7%94%A8%E7%9A%84%E3%81%AA%E8%A8%AD%E5%AE%9A%E3%81%A8%E6%9C%80%E9%81%A9%E5%8C%96%E6%8A%80%E6%B3%95)
      - [ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚ã®æœ€é©åŒ–](#%E3%83%A2%E3%83%87%E3%83%AB%E3%83%AD%E3%83%BC%E3%83%89%E6%99%82%E3%81%AE%E6%9C%80%E9%81%A9%E5%8C%96)
      - [ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–](#%E3%83%90%E3%83%83%E3%83%81%E5%87%A6%E7%90%86%E3%81%AE%E6%9C%80%E9%81%A9%E5%8C%96)
    - [å‚è€ƒ](#%E5%8F%82%E8%80%83)


---

# Transformers ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

Transformers ã¯ã€PyTorch, TensorFlow, JAX ã«å¯¾å¿œã—ãŸæ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€æœ€å…ˆç«¯ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã‚„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€éŸ³å£°èªè­˜ãªã©ã•ã¾ã–ã¾ãªåˆ†é‡ã§ã®ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€æŸ”è»Ÿãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ç›¸äº’é‹ç”¨æ€§ã¨æœ¬ç•ªç’°å¢ƒå‘ã‘ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ©Ÿèƒ½ï¼ˆONNX ã‚„ TorchScript å½¢å¼ã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼‰ã‚’æä¾›ã—ã¾ã™ã€‚


## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
### pipã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

- å‰æã¨ã—ã¦ã€Pytorchã¾ãŸã¯tensorflowã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€ã•ã‚‰ã«ã€flaxã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚


ã“ã‚Œã§ã€æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§ğŸ¤— Transformersã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸ:

```
pip install transformers
```

CPUå¯¾å¿œã®ã¿å¿…è¦ãªå ´åˆã€ğŸ¤— Transformersã¨Deep Learningãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’1è¡Œã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¦ä¾¿åˆ©ã§ã™ã€‚ä¾‹ãˆã°ã€ğŸ¤— Transformersã¨PyTorchã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ä¸€ç·’ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™:

```
pip install transformers[torch]
```

ğŸ¤— Transformersã¨TensorFlow 2.0:

```
pip install transformers[tf-cpu]
```

ğŸ¤— Transformersã¨Flax:

```
pip install transformers[flax]
```

æœ€å¾Œã«ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ğŸ¤— TransformersãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™:

```
python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"
```

ãã®å¾Œã€ãƒ©ãƒ™ãƒ«ã¨ã‚¹ã‚³ã‚¢ãŒå‡ºåŠ›ã•ã‚Œã¾ã™:

```
[{'label': 'POSITIVE', 'score': 0.9998704791069031}]
```

### ç·¨é›†å¯èƒ½ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

å¿…è¦ã«å¿œã˜ã¦ã€ç·¨é›†å¯èƒ½ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã—ã¾ã™:

- ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®`main`ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ã„ã¾ã™ã€‚
- ğŸ¤— Transformersã«ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆã—ã€ã‚³ãƒ¼ãƒ‰ã®å¤‰æ›´ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ¬ãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¦ã€ğŸ¤— Transformersã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™:

```
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š
- å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ `~/.cache/huggingface/hub` ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚
- Windowsã§ã¯ã€`C:\Users\username\.cache\huggingface\hub` ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã™ã€‚
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã§å¤‰æ›´å¯èƒ½ï¼ˆå„ªå…ˆé †ä½é †ï¼‰:
  1. `HF_HUB_CACHE` or `TRANSFORMERS_CACHE`
  2. `HF_HOME`
  3. `XDG_CACHE_HOME` + `/huggingface`

> âš ï¸ éå»ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ `PYTORCH_TRANSFORMERS_CACHE` ã¾ãŸã¯ `PYTORCH_PRETRAINED_BERT_CACHE` ã‚’ä½¿ç”¨ã—ã¦ã„ãŸå ´åˆã€ãã‚Œã‚‰ãŒå¼•ãç¶šãä½¿ç”¨ã•ã‚Œã¾ã™ï¼ˆæ˜ç¤ºçš„ã« `TRANSFORMERS_CACHE` ã‚’è¨­å®šã—ã¦ã„ãªã„é™ã‚Šï¼‰ã€‚


### ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ã‚‚å‹•ä½œã•ã›ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™:
  - `HF_HUB_OFFLINE=1`: Hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãªã„
  - `HF_DATASETS_OFFLINE=1`: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¯¾å¿œ

ä¾‹:
```bash
HF_DATASETS_OFFLINE=1 HF_HUB_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...
```


### ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ã®ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆ©ç”¨æ–¹æ³•
#### **Model Hub UI** ã‹ã‚‰æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä¸Šã® â†“ ã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã€‚

#### PreTrainedModel.from_pretrained() & save_pretrained() ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
å‰ã‚‚ã£ã¦ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†ä¿å­˜:
```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")

tokenizer.save_pretrained("./your/path/bigscience_t0")
model.save_pretrained("./your/path/bigscience_t0")
```

ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§å†èª­ã¿è¾¼ã¿:
```python
tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
model = AutoModel.from_pretrained("./your/path/bigscience_t0")
```

#### huggingface_hubãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ãªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
```bash
python -m pip install huggingface_hub
```

ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰:
```python
from huggingface_hub import hf_hub_download

hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
```

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã«ãƒ­ãƒ¼ãƒ‰:
```python
from transformers import AutoConfig

config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

## ä½¿ã„æ–¹

### ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¦‚è¦

HuggingFace Transformers ã¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€ä»¥ä¸‹ã®ã‚ˆã†ãªä¸»è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™:

- **`AutoTokenizer`**: ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†è©ã‚„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
- **`AutoModel`**: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®åŸºæœ¬ã‚¯ãƒ©ã‚¹ã§ã™ã€‚
- **`Trainer`, `TrainingArguments`**: ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ãŸã‚ã®é«˜ãƒ¬ãƒ™ãƒ«ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
- **`Pipeline`**: å‰å‡¦ç†ã‹ã‚‰æ¨è«–ã€å¾Œå‡¦ç†ã¾ã§ã®å…¨ãƒ•ãƒ­ãƒ¼ã‚’ã‚«ãƒ—ã‚»ãƒ«åŒ–ã—ã¦ãŠã‚Šã€ç´ æ—©ãé–‹ç™ºã‚’é–‹å§‹ã§ãã¾ã™ã€‚

#### ç°¡å˜ãªæ¨è«–ã‚³ãƒ¼ãƒ‰ä¾‹:
```python
from transformers import AutoTokenizer, AutoModel

def basic_usage_example():
    # Tokenizerã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')
    model = AutoModel.from_pretrained('bert-base-chinese')
    
    # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
    text = "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œï¼"
    inputs = tokenizer(text, return_tensors="pt")  # PyTorchãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã™
    
    # æ¨è«–å®Ÿè¡Œ
    outputs = model(**inputs)
    
    # éš ã‚Œå±¤ã®æœ€çµ‚å‡ºåŠ›ã‚’è¿”ã™
    return outputs.last_hidden_state
```

### Pipeline

`pipeline` ã¯éå¸¸ã«ç°¡å˜ã«ã•ã¾ã–ã¾ãªNLPã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã™ã€‚

#### ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰:
```python
from transformers import pipeline

def pipeline_examples():
    """ä»£è¡¨çš„ãªã‚¿ã‚¹ã‚¯ã®Pipelineä¾‹"""

    # æ„Ÿæƒ…åˆ†æ
    sentiment_analyzer = pipeline("sentiment-analysis")
    result = sentiment_analyzer("ã“ã®è£½å“ã¯ã¨ã¦ã‚‚ä½¿ã„ã‚„ã™ã„ï¼")
    print(f"æ„Ÿæƒ…åˆ†æçµæœï¼š{result}")
    
    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆGPT-2ï¼‰
    generator = pipeline("text-generation", model="gpt2-chinese")
    text = generator("äººå·¥çŸ¥èƒ½ã¯ä»Š", max_length=50)
    print(f"ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼š{text}")
    
    # å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰
    ner = pipeline("ner", model="bert-base-chinese")
    entities = ner("è¯ç‚ºã®æœ¬ç¤¾ã¯æ·±åœ³ã«ã‚ã‚Šã¾ã™")
    print(f"èªè­˜ã•ã‚ŒãŸå›ºæœ‰åè©ï¼š{entities}")
    
    # è³ªå•å¿œç­”ã‚·ã‚¹ãƒ†ãƒ 
    qa = pipeline("question-answering", model="bert-base-chinese")
    context = "åŒ—äº¬ã¯ä¸­å›½ã®é¦–éƒ½ã§ã‚ã‚Šã€ä¸Šæµ·ã¯æœ€å¤§ã®çµŒæ¸ˆéƒ½å¸‚ã§ã™ã€‚"
    question = "ä¸­å›½ã®é¦–éƒ½æ˜¯ã¯ã©ã“ã§ã™ã‹ï¼Ÿ"
    answer = qa(question=question, context=context)
    print(f"è³ªå•å¿œç­”çµæœï¼š{answer}")

if __name__ == "__main__":
    pipeline_examples()
```

### Tokenizers

Tokenizers ã¯è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã«ãŠã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹å½¢å¼ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®åŸºæœ¬çš„ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã™ã€‚  
HuggingFace Transformers ã§ã¯ã€å¤šæ§˜ãªè¨€èªã‚„ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã—ãŸæŸ”è»Ÿã§é«˜æ€§èƒ½ãª `Tokenizer API` ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

#### åˆ†è©å™¨ã®ä¸»ãªæ©Ÿèƒ½

##### **Tokenize**
- ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã€ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã€æ–‡å­—ãªã©ã®ã€Œãƒˆãƒ¼ã‚¯ãƒ³ã€ã«åˆ†å‰²ã—ã¾ã™ã€‚
- ä¾‹: `"ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆ"` â†’ `["ã“ã‚Œ", "ã¯", "ãƒ†ã‚¹ãƒˆ"]`

```python
tokens = tokenizer.tokenize("ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆ")
```

##### **Encode**
- ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•°å€¤ IDï¼ˆèªå½™IDï¼‰ã«å¤‰æ›ã—ã¾ã™ã€‚
- ä¾‹: `["ã“ã‚Œ", "ã¯", "ãƒ†ã‚¹ãƒˆ"]` â†’ `[345, 890, 1234]`

```python
token_ids = tokenizer.convert_tokens_to_ids(tokens)
```

##### **Encode + ç‰¹æ®Šã‚¿ã‚°ä»˜åŠ **
- å…¥åŠ›ã«ç‰¹æ®Šã‚¿ã‚°ï¼ˆ[CLS], [SEP]ãªã©ï¼‰ã‚’è¿½åŠ ã—ã€ãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã§å‡ºåŠ›ã—ã¾ã™ã€‚

```python
encoded = tokenizer(text, return_tensors="pt")
```

##### **Decode**
- æ•°å€¤ ID ã‚’å†ã³äººé–“å¯èª­ãªãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã—ã¾ã™ã€‚

```python
decoded_text = tokenizer.decode(encoded["input_ids"][0])
```

#### åˆ†è©å™¨ã®é«˜éšæ©Ÿèƒ½

#####  ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨èªå½™æƒ…å ±

- **[CLS]**: åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨ã®ç‰¹åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³
- **[SEP]**: æ–‡ã®åŒºåˆ‡ã‚Šã‚’ç¤ºã™ãƒˆãƒ¼ã‚¯ãƒ³

```python
print(f"CLSæ ‡è®°: {tokenizer.cls_token}")      # [CLS]
print(f"SEPæ ‡è®°: {tokenizer.sep_token}")      # [SEP]
print(f"è¯è¡¨å¤§å°: {len(tokenizer)}")          # èªå½™æ•°
print(f"ç‰¹æ®Šæ ‡è®°æ˜ å°„: {tokenizer.special_tokens_map}")
```

##### ãƒãƒƒãƒå‡¦ç†ã¨é•·æ–‡å¯¾å¿œ

è¤‡æ•°æ–‡ã‚’ä¸€åº¦ã«å‡¦ç†ã—ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¨åˆ‡ã‚Šè©°ã‚ã‚‚è‡ªå‹•åŒ–ã§ãã¾ã™ã€‚

```python
batch_encoding = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
```

é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æœ€å¤§é•·ã§è‡ªå‹•çš„ã«åˆ‡ã‚Šè©°ã‚ã¦å‡¦ç†ã—ã¾ã™ã€‚

```python
truncated = tokenizer(long_text, max_length=128, truncation=True)
```

#### å®Ÿè£…ä¾‹

```python
from transformers import AutoTokenizer

def tokenizer_basics():
    # åˆ†è©å™¨ãƒ­ãƒ¼ãƒ‰
    tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
    text = "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆ"

    # 1. åˆ†è©
    tokens = tokenizer.tokenize(text)
    print(f"Result: {tokens}")

    # 2. Token IDs ã¸å¤‰æ›
    token_ids = tokenizer.convert_tokens_to_ids(tokens)
    print(f"Token IDs: {token_ids}")

    # 3. ç·¨ç¢¼ï¼ˆç‰¹æ®Šã‚¿ã‚°å«ã‚€ï¼‰
    encoded = tokenizer(text, return_tensors="pt")
    print(f"Encoding result: {encoded}")

    # 4. è§£ç 
    decoded = tokenizer.decode(encoded["input_ids"][0])
    print(f"Decoding result: {decoded}")

# ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±
def tokenizer_special_tokens():
    tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
    print(f"CLS map: {tokenizer.cls_token}")
    print(f"SEP map: {tokenizer.sep_token}")
    print(f"tokenizer length: {len(tokenizer)}")
    print(f"spcial token map: {tokenizer.special_tokens_map}")

# ãƒãƒƒãƒãƒ»é•·æ–‡å‡¦ç†
def batch_and_long_text_processing():
    tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
    texts = ["this is the first text", "this is the second text"]
    batch_encoding = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    print(f"batch processing result: {batch_encoding}")

    long_text = "this is a " + "very very very very very very very very very very " * 50 + "long textã€‚"""
    truncated = tokenizer(long_text, max_length=128, truncation=True)
    print(f"Result: {truncated['input_ids']}")

# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    tokenizer_basics()
    tokenizer_special_tokens()
    batch_and_long_text_processing()
```

### Models
HuggingFace Transformers ã§ã¯ã€**ãƒ¢ãƒ‡ãƒ« (`Model`) ãŒæ¨è«–ã‚„å­¦ç¿’ã®ä¸­å¿ƒã¨ãªã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**ã§ã™ã€‚  
ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã™ã‚‹çµ±ä¸€ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¦ãŠã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ãªä¸»è¦ãªã‚¯ãƒ©ã‚¹ãŒã‚ã‚Šã¾ã™ï¼š

- `AutoModel`: åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ ï¼ˆã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ã«æ±ç”¨çš„ã«ä½¿ç”¨å¯èƒ½ï¼‰
- ã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹ãƒ¢ãƒ‡ãƒ«:
  - `AutoModelForSequenceClassification`: æ–‡ç« åˆ†é¡
  - `AutoModelForQuestionAnswering`: è³ªå•å¿œç­”
  - `AutoModelForTokenClassification`: å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰
  - `AutoModelForSeq2SeqLM`: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ç”Ÿæˆï¼ˆç¿»è¨³ãƒ»è¦ç´„ãªã©ï¼‰

#### æ–‡æ›¸åˆ†é¡ï¼ˆSequence Classificationï¼‰

BERTãªã©ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦æ–‡æ›¸ã®æ„Ÿæƒ…æ¥µæ€§ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰ãªã©ã‚’åˆ†é¡ã§ãã¾ã™ã€‚

```python
from transformers import AutoModelForSequenceClassification
import torch

def text_classification_example():
    # åˆ†é¡ç”¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ©ãƒ™ãƒ«æ•°=2ï¼‰
    model = AutoModelForSequenceClassification.from_pretrained("bert-base-chinese", num_labels=2)

    # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
    text = "ã“ã®å•†å“ã¯éå¸¸ã«ä¾¿åˆ©ã§ã™ã€‚"
    inputs = tokenizer(text, return_tensors="pt")

    # æ¨è«–
    outputs = model(**inputs)
    probabilities = torch.softmax(outputs.logits, dim=-1)

    print(f"Result: {probabilities}")
```

#### è³ªå•å¿œç­”ï¼ˆQuestion Answeringï¼‰

BERTãƒ™ãƒ¼ã‚¹ã® QA ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ã‚ã‚‹æ–‡ç« ã®ä¸­ã‹ã‚‰è³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆã‚’æŠ½å‡ºã—ã¾ã™ã€‚

```python
from transformers import AutoModelForQuestionAnswering

def question_answering_example():
    # è³ªå•å¿œç­”ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    model = AutoModelForQuestionAnswering.from_pretrained("bert-base-chinese")

    # è³ªå•ã¨ä¸Šä¸‹æ–‡
    context = "This is an apple. Apple is a fruit."

    question = "What is this?"

    # å…¥åŠ›ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    inputs = tokenizer(question, context, return_tensors="pt")

    # æ¨è«–
    outputs = model(**inputs)

    start_index = torch.argmax(outputs.start_logits)
    end_index = torch.argmax(outputs.end_logits)

    answer = tokenizer.decode(inputs["input_ids"][0][start_index:end_index+1])
    print(f"ç­”æ¡ˆ: {answer}")
```

#### ä¸»ãªãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ä¸€è¦§

| ã‚¯ãƒ©ã‚¹å                             | ä½¿ç”¨ç”¨é€”                       |
| ------------------------------------ | ------------------------------ |
| `AutoModel`                          | ä¸€èˆ¬çš„ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹è¡¨ç¾ã®å–å¾—   |
| `AutoModelForSequenceClassification` | æ–‡ç« åˆ†é¡ï¼ˆæ„Ÿæƒ…åˆ†æãªã©ï¼‰       |
| `AutoModelForQuestionAnswering`      | è³ªå•å¿œç­”ï¼ˆSQuADãªã©ï¼‰          |
| `AutoModelForTokenClassification`    | å›ºæœ‰è¡¨ç¾æŠ½å‡ºï¼ˆNERï¼‰            |
| `AutoModelForSeq2SeqLM`              | ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é–“å¤‰æ›ï¼ˆç¿»è¨³ã€è¦ç´„ï¼‰ |
| `AutoModelForCausalLM`               | è¨€èªç”Ÿæˆï¼ˆGPTç³»ï¼‰              |


#### å®Ÿè£…ãƒ•ãƒ­ãƒ¼ã¾ã¨ã‚

1. **åˆ†è©å™¨ã®ãƒ­ãƒ¼ãƒ‰**: `AutoTokenizer.from_pretrained(...)`
2. **ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰**: `AutoModel.from_pretrained(...)` or ã‚¿ã‚¹ã‚¯å°‚ç”¨ãƒ¢ãƒ‡ãƒ«
3. **å…¥åŠ›å‡¦ç†**: `tokenizer(text, return_tensors="pt")`
4. **æ¨è«–å®Ÿè¡Œ**: `model(**inputs)`
5. **çµæœã®è§£é‡ˆ**: logits, hidden states, etc.


### Configuration

`Configuration` ã¯ HuggingFace Transformers ã«ãŠã‘ã‚‹ **ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã¨æŒ™å‹•ã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®é‡è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**ã§ã™ã€‚  
ã“ã®ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã¨ï¼š

- ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå±¤æ•°ã€éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºãªã©ï¼‰ã‚’ç¢ºèªã§ãã¾ã™ã€‚
- ã‚¿ã‚¹ã‚¯ã‚„ãƒªã‚½ãƒ¼ã‚¹ã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒ æ§‹æˆã‚’ä½œæˆã§ãã¾ã™ã€‚
- è¨­å®šã‚’ä¿å­˜ãƒ»å†åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€è¤‡æ•°ç’°å¢ƒã§ã®ä¸€è²«æ€§ã‚’ç¶­æŒã§ãã¾ã™ã€‚

#### åŸºæœ¬è¨­å®šã®ãƒ­ãƒ¼ãƒ‰ã¨ç¢ºèª

å„äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã¯ã€ãã®ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚  
ã“ã®è¨­å®šã¯ `AutoConfig.from_pretrained(...)` ã‚’ä½¿ã£ã¦å–å¾—ã§ãã¾ã™ã€‚

```python
from transformers import AutoConfig

def load_model_config():
    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰
    config = AutoConfig.from_pretrained("bert-base-chinese")

    # ä¸»ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¡¨ç¤º
    print(f"éšè—å±‚å¤§å°: {config.hidden_size}")              # éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°
    print(f"æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}")      # æ³¨æ„åŠ›ãƒ˜ãƒƒãƒ‰æ•°
    print(f"éšè—å±‚æ•°é‡: {config.num_hidden_layers}")        # Transformer å±¤æ•°
    print(f"æœ€å¤§ä½ç½®ç¼–ç : {config.max_position_embeddings}")# æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
```

> ã“ã®ã‚ˆã†ãªè¨­å®šæƒ…å ±ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ãŸã‚ã€**ã‚¿ã‚¹ã‚¯ã«åˆã‚ã›ãŸèª¿æ•´ãŒå¿…è¦**ã§ã™ã€‚

#### ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã®ä½œæˆ

ç‰¹å®šã®ç”¨é€”ã‚„åˆ¶ç´„ï¼ˆä¾‹ï¼šå°‘ãªã„GPUãƒ¡ãƒ¢ãƒªã€é«˜é€Ÿæ¨è«–ï¼‰ã«åˆã‚ã›ã¦ã€**ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’å¤‰æ›´ã—ãŸç‹¬è‡ªã®è¨­å®š**ã‚’ä½œæˆã§ãã¾ã™ã€‚

```python
from transformers import PretrainedConfig, AutoModel

def create_custom_config():
    # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã®ä½œæˆ
    custom_config = PretrainedConfig(
        vocab_size=21128,                # èªå½™æ•°
        hidden_size=512,                 # éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºï¼ˆå°ã•ã‚ï¼‰
        num_hidden_layers=6,             # å±¤æ•°ã‚’æ¸›ã‚‰ã—ã¦è»½é‡åŒ–
        num_attention_heads=8,           # æ³¨æ„åŠ›ãƒ˜ãƒƒãƒ‰æ•°ã‚‚æ¸›ã‚‰ã™
        intermediate_size=2048,          # ä¸­é–“å±¤ã®ã‚µã‚¤ã‚º
        max_position_embeddings=256,     # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’çŸ­ã
    )

    # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    model = AutoModel.from_config(custom_config)
    print(f"ãƒ¢ãƒ‡ãƒ«é…ç½®æƒ…å ±: {model.config}")
```

> ã“ã®ã‚ˆã†ã«ã—ã¦ã€å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚„è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¼ãƒ­ã‹ã‚‰æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

#### è¨­å®šã®ä¿å­˜ã¨å†èª­ã¿è¾¼ã¿

ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã“ã¨ã§ã€**å¾Œã§å†åˆ©ç”¨ã—ãŸã‚Šå…±æœ‰ã—ãŸã‚Š**ã§ãã¾ã™ã€‚

```python
def save_and_load_config():
    # äº‹å‰å­¦ç¿’æ¸ˆã¿è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰
    config = AutoConfig.from_pretrained("bert-base-chinese")

    # å¿…è¦ã«å¿œã˜ã¦è¨­å®šã‚’å¤‰æ›´
    config.hidden_dropout_prob = 0.2
    config.attention_probs_dropout_prob = 0.2

    # è¨­å®šã‚’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
    config.save_pretrained("./custom_config")

    # ä¿å­˜ã—ãŸè¨­å®šã‚’å†ãƒ­ãƒ¼ãƒ‰
    new_config = AutoConfig.from_pretrained("./custom_config")
    print(f"åŠ è½½çš„é…ç½®: {new_config}")
```

> ã“ã‚Œã«ã‚ˆã‚Šã€è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã«ã€**è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆconfig.jsonï¼‰ã¨ã—ã¦ä¿å­˜ãƒ»å…±æœ‰**ã§ãã¾ã™ã€‚

#### ä¸»ãªè¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸€è¦§

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å                   | å†…å®¹                      |
| ------------------------------ | ------------------------- |
| `vocab_size`                   | èªå½™æ•°                    |
| `hidden_size`                  | éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°            |
| `num_hidden_layers`            | Transformer å±¤ã®æ•°        |
| `num_attention_heads`          | æ³¨æ„åŠ›ãƒ˜ãƒƒãƒ‰æ•°            |
| `intermediate_size`            | Feed-forward å±¤ã®ä¸­é–“æ¬¡å…ƒ |
| `max_position_embeddings`      | æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·          |
| `hidden_dropout_prob`          | ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡          |
| `attention_probs_dropout_prob` | æ³¨æ„åŠ›ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡    |

### å®Ÿç”¨çš„ãªè¨­å®šã¨æœ€é©åŒ–æŠ€æ³•

#### ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚ã®æœ€é©åŒ–
ä»¥ä¸‹ã®æ–¹æ³•ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨æ¨è«–é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

```python
from transformers import AutoModel
import torch

def setup_optimization():
    """ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–è¨­å®š"""
    model = AutoModel.from_pretrained(
        "bert-base-chinese",
        device_map="auto",         # è‡ªå‹•çš„ã«GPU/CPUã«å‰²ã‚Šå½“ã¦
        torch_dtype=torch.float16, # åŠç²¾åº¦ã‚’ä½¿ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
        low_cpu_mem_usage=True     # CPUãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’æŠ‘ãˆã‚‹
    )
    model.eval()  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã¸åˆ‡ã‚Šæ›¿ãˆ
    return model
```

#### ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–
å¤§è¦æ¨¡ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†éš›ã«ã¯ã€ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Šå‡¦ç†é€Ÿåº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚é•·æ–‡ã®åˆ†å‰²ã‚‚è‡ªå‹•åŒ–ã—ã¦ã„ã¾ã™ã€‚

```python
from typing import List

def batch_process(texts: List[str], batch_size: int, max_length: int) -> List[List[str]]:
    """é•·æ–‡ã‚’åˆ†å‰²ã—ã€æŒ‡å®šãƒãƒƒãƒã‚µã‚¤ã‚ºã§å‡¦ç†ã™ã‚‹"""
    processed_texts = []
    for text in texts:
        if len(text) > max_length:
            chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]
            processed_texts.extend(chunks)
        else:
            processed_texts.append(text)
    
    return [processed_texts[i:i+batch_size] for i in range(0, len(processed_texts), batch_size)]
```


### å‚è€ƒ
- Hubã‹ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è©³ç´°ã«ã¤ã„ã¦ã¯ [Transformers official doc](https://huggingface.co/docs/transformers) ã‚’å‚ç…§ã€‚
- [Hugging Face Tokenizers Documentation](https://huggingface.co/docs/transformers/tokenizer_summary)