---
title: pytorchで並列的に学習を行う方法
date: 2022-8-12 11:15:00
categories: [AI]
tags:
  [
    Deep Learning,
    PyTorch,
    Python,
    機械学習,
    AI,
    人工知能,
    深層学習,
  ]
lang: ja
---

## 1台のマシンに複数のGPUを使う場合
1台のマシンに1枚のカードの場合、情報はすべて1台のマシンにあり、分散はない。 
分散トレーニングでは、情報は「分散」され、その分散の仕方の違いを「並列性」と呼ぶことが多い。 通常、分散は「データ並列」(Data Parallelism)と「モデル並列」(Model Parallelism)に分類されるのが通例である。

### データ並列
データ並列処理では、サンプルデータはスライスされ、スライスされたデータは各トレーニングノードに送られ、そこで完全なモデルに対して実行され、複数のノードからの情報がマージされる。


### モデル並列
