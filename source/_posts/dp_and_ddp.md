---
title: pytorchで並列的に学習を行う方法
date: 2022-9-23 11:15:00
categories: [AI]
tags:
  [
    Deep Learning,
    PyTorch,
    Python,
    機械学習,
    AI,
    人工知能,
    深層学習,
  ]
lang: ja
---

## 1台のマシンに複数のGPUを使う場合
1台のマシンに1枚のカードの場合、情報はすべて1台のマシンにあり、分散はない。 
分散トレーニングでは、情報は「分散」され、その分散の仕方の違いを「並列性」と呼ぶことが多い。 通常、分散は「データ並列」(Data Parallelism)と「モデル並列」(Model Parallelism)に分類されるのが通例である。

### データ並列
データ並列処理では、サンプルデータはスライスされ、スライスされたデータは各トレーニングノードに送られ、そこで完全なモデルに対して実行され、複数のノードからの情報がマージされる。

![Data Parallelism](/assert/dp_and_ddp/data_pa.png)

**GPUの観点から説明すると**、各GPUに同じ構造のモデルを複製し、入力データをミニバッチ単位で分割して並列処理します。その後、各GPUで計算された勾配を統合・同期することで、全体の学習を進めます。  
具体的な手順は以下の通りです：
1. **データ分割**：CPUが各GPUに異なるミニバッチデータを配布。
2. **並列計算**：各GPUでモデルの順伝播・逆伝播を独立して実行。
3. **勾配同期**：各GPU間で勾配を収集（AllReduce）し、パラメータを更新。

#### なぜデータ並列が必要か
- **訓練速度の加速**：データ量が増加するにつれて、単一GPUでは訓練時間が過長になるため、複数GPUで分散処理することで線形近似の高速化を実現します。
- **リソース効率化**：大規模データセット（例：ImageNet）や複雑なモデル（例：ResNet50）において、限られたハードウェアリソースで効率的な学習を可能にします。



### モデル並列
モデル並列では、モデルはスライスされ、完全なデータが各トレーニングノードに送られ、そこでスライスされたモデルに対して実行され、複数のノードからの結果がマージされる。
![Model Parallelism](/assert/dp_and_ddp/model_pa.png)