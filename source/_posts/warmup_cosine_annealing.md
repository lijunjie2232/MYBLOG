---
title: Warm up ã¨ Cosine Anneal LR ã®çµ„ã¿åˆã‚ã›


date: 2023-10-21 12:00:00
categories: [AI]
tags: [Deep Learning, PyTorch, Python, æ©Ÿæ¢°å­¦ç¿’, AI, äººå·¥çŸ¥èƒ½, æ·±å±¤å­¦ç¿’]
lang: ja

description: ã‚
---

## ç›®æ¬¡

---

## Linear Warmup ã¨ã¯

- **Linear Warmup**: å­¦ç¿’é–‹å§‹æ™‚ã«å­¦ç¿’ç‡ã‚’ 0 ã‹ã‚‰å¾ã€…ã«å¢—åŠ ã•ã›ã‚‹æ‰‹æ³•ã€‚åˆæœŸã®å¤§ããªæ›´æ–°ã«ã‚ˆã‚‹ä¸å®‰å®šæ€§ã‚’è»½æ¸›ã€‚


## Cosine Annealing ã¨ã¯

- **Cosine Annealing**: å­¦ç¿’ç‡ã‚’ã‚³ã‚µã‚¤ãƒ³é–¢æ•°ã®ã‚ˆã†ã«æ¸›å°‘ã•ã›ãªãŒã‚‰æœ€é©è§£ã«è¿‘ã¥ãæ‰‹æ³•ã€‚å‘¨æœŸçš„ã«å¾©æ´»ã•ã›ã‚‹ `SGDR`ï¼ˆStochastic Gradient Descent with Warm Restartsï¼‰ã¨ã—ã¦ã‚‚çŸ¥ã‚‰ã‚Œã‚‹ã€‚

### åŸºæœ¬ã®æ›´æ–°å¼

<center>$ \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 + \cos\left(\frac{T_{cur}}{T_{max}} \pi\right)\right), T_{cur} \ne (2k+1)T_{max} $</center>
<center>$ \eta_{t+1} = \eta_t + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 - \cos\left(\frac{T_{cur}}{T_{max}} \pi\right)\right), T_{cur} = (2k+1)T_{max} $</center>

- $t$: ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
- $k$: ç¾åœ¨ã®å‘¨æœŸæ•°ï¼ˆæ•´æ•°ï¼‰
- $\eta_{t}$: ç¾åœ¨ã®å­¦ç¿’ç‡
- $\eta_{min}$: æœ€å°å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0ï¼‰
- $\eta_{max}$: åˆæœŸå­¦ç¿’ç‡ï¼ˆoptimizer ã§è¨­å®šã•ã‚ŒãŸå€¤ï¼‰
- $T_{cur}$: ç¾åœ¨ã® epoch æ•° or step æ•°
- $T_{max}$: æœ€å¤§å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•° or ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆåŠå‘¨æœŸï¼‰

### ã‚³ãƒ¼ãƒ‰ä¾‹

```python
torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max,
    eta_min=0,
    last_epoch=-1
)
```

## ğŸ“š å‚è€ƒãƒªãƒ³ã‚¯

- [PyTorch CosineAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)
