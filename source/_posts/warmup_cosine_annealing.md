---
title: Pytorch ã§ Warm up ã¨ Cosine Anneal LR ã®çµ„ã¿åˆã‚ã›

date: 2023-10-21 12:00:00
categories: [AI]
tags: [Deep Learning, PyTorch, Python, æ©Ÿæ¢°å­¦ç¿’, AI, äººå·¥çŸ¥èƒ½, æ·±å±¤å­¦ç¿’]
lang: ja

description: ã‚
---

## ç›®æ¬¡

---

## Linear Warmup ã¨ã¯

- **Linear Warmup**: å­¦ç¿’é–‹å§‹æ™‚ã«å­¦ç¿’ç‡ã‚’ 0 ã‹ã‚‰å¾ã€…ã«å¢—åŠ ã•ã›ã‚‹æ‰‹æ³•ã€‚åˆæœŸã®å¤§ããªæ›´æ–°ã«ã‚ˆã‚‹ä¸å®‰å®šæ€§ã‚’è»½æ¸›ã€‚

## ãªãœ Warmup ãŒå¿…è¦

- å­¦ç¿’åˆæœŸã«é‡ã¿ãŒä¸å®‰å®šãªãŸã‚ã€å¤§ããªå­¦ç¿’ç‡ã‚’ä½¿ã†ã¨ç™ºæ•£ã—ã‚„ã™ã„
- å­¦ç¿’ç‡ã‚’å¾ã€…ã«å¢—åŠ ã•ã›ã‚‹ã“ã¨ã§ã€å®‰å®šã—ã¦åæŸã™ã‚‹

## åŸºæœ¬çš„ãª Warmup å®Ÿè£…æ–¹æ³•

| ã‚¯ãƒ©ã‚¹å                                               | èª¬æ˜                                                                    |
| ------------------------------------------------------ | ----------------------------------------------------------------------- |
| `torch.optim.lr_scheduler.LambdaLR`                    | ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ã§å­¦ç¿’ç‡ã‚’å¤‰æ›´ã§ãã‚‹ï¼ˆwarmup å®Ÿè£…ã«æœ€é©ï¼‰                   |
| `torch.optim.lr_scheduler.LinearLR`                    | åˆæœŸå­¦ç¿’ç‡ã‹ã‚‰ç·šå½¢çš„ã«ä¼¸ã°ã™ï¼ˆv1.12+ï¼‰                                  |
| `torch.optim.lr_scheduler.ConstantLRWithWarmup`        | åˆæœŸæœŸé–“ã¯ä¸€å®šã®å­¦ç¿’ç‡ã‚’ä½¿ç”¨ï¼ˆHuggingFace Transformers ãªã©ã§ä½¿ã‚ã‚Œã‚‹ï¼‰ |
| `torch.optim.lr_scheduler.CosineAnnealingWarmRestarts` | ä½™å¼¦é€€ç«ï¼‹å†ã‚¹ã‚¿ãƒ¼ãƒˆï¼‹ warmup çš„æŒ™å‹•                                    |

## ã‚³ãƒ¼ãƒ‰ä¾‹

```python
import torch
from torch.optim.lr_scheduler import LambdaLR


def get_warmup_scheduler(optimizer, warmup_steps):
    def lr_lambda(current_step: int):
        if current_step < warmup_steps:
            return float(current_step) / float(max(1, warmup_steps))
        return 1.0

    return LambdaLR(optimizer, lr_lambda)

```

## Cosine Annealing ã¨ã¯

- **Cosine Annealing**: å­¦ç¿’ç‡ã‚’ã‚³ã‚µã‚¤ãƒ³é–¢æ•°ã®ã‚ˆã†ã«æ¸›å°‘ã•ã›ãªãŒã‚‰æœ€é©è§£ã«è¿‘ã¥ãæ‰‹æ³•ã€‚å‘¨æœŸçš„ã«å¾©æ´»ã•ã›ã‚‹ `SGDR`ï¼ˆStochastic Gradient Descent with Warm Restartsï¼‰ã¨ã—ã¦ã‚‚çŸ¥ã‚‰ã‚Œã‚‹ã€‚

### åŸºæœ¬ã®æ›´æ–°å¼

<center>$ \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 + \cos\left(\frac{T_{cur}}{T_{max}} \pi\right)\right), T_{cur} \ne (2k+1)T_{max} $</center>
<center>$ \eta_{t+1} = \eta_t + \frac{1}{2}(\eta_{max} - \eta_{min}) \left(1 - \cos\left(\frac{T_{cur}}{T_{max}} \pi\right)\right), T_{cur} = (2k+1)T_{max} $</center>

- $t$: ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
- $k$: ç¾åœ¨ã®å‘¨æœŸæ•°ï¼ˆæ•´æ•°ï¼‰
- $\eta_{t}$: ç¾åœ¨ã®å­¦ç¿’ç‡
- $\eta_{min}$: æœ€å°å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0ï¼‰
- $\eta_{max}$: åˆæœŸå­¦ç¿’ç‡ï¼ˆoptimizer ã§è¨­å®šã•ã‚ŒãŸå€¤ï¼‰
- $T_{cur}$: ç¾åœ¨ã® epoch æ•° or step æ•°
- $T_{max}$: æœ€å¤§å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•° or ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆåŠå‘¨æœŸï¼‰

## LambdaLR ã§ warmup ã¨ cosine annealing ã‚’çµ„åˆ

```python
import math  # æ•°å­¦é–¢æ•°ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import torch  # PyTorchãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from torchvision.models import resnet18  # ResNet-18ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

model = resnet18(pretrained=True)	# äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ResNet18ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
optimizer = torch.optim.SGD(params=[	# SGDã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’åˆæœŸåŒ–ã—ã€2ã¤ã®param_groupã‚’è¨­å®š
    {'params': model.layer2.parameters()},	# layer2ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®lr (0.1)
    {'params': model.layer3.parameters(), 'lr':0.2},	# layer3ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯å€‹åˆ¥ã®lr (0.2)
], lr=0.1)	# åŸºæœ¬å­¦ç¿’ç‡ (base_lr) ã‚’0.1ã«è¨­å®š

# warm upã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã‚’è¨­å®š
warm_up_iter = 10
T_max = 50	# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ç·ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ï¼ˆå‘¨æœŸï¼‰
lr_max = 0.1	# æœ€å¤§å­¦ç¿’ç‡
lr_min = 1e-5	# æœ€å°å­¦ç¿’ç‡

# param_groups[0] (model.layer2) ã®å­¦ç¿’ç‡èª¿æ•´é–¢æ•°: Warm up + Cosine Annealing
lambda0 = lambda cur_iter: (
        cur_iter / warm_up_iter
        if cur_iter < warm_up_iter
        else (
            lr_min
            + 0.5
            * (lr_max - lr_min)
            * (
                1.0
                + math.cos((cur_iter - warm_up_iter) / (T_max - warm_up_iter) * math.pi)
            )
        )
        / 0.1
    )

# param_groups[1] (model.layer3) ã®å­¦ç¿’ç‡ã¯å¤‰æ›´ã—ãªã„
lambda1 = lambda cur_iter: 1

# LambdaLRã‚’ä½¿ç”¨ã—ã¦ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’è¨­å®š
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda0, lambda1])

for epoch in range(50):	# å­¦ç¿’ã‚¨ãƒãƒƒã‚¯ã®ãƒ«ãƒ¼ãƒ—
    print(optimizer.param_groups[0]['lr'], optimizer.param_groups[1]['lr'])	# ç¾åœ¨ã®å„param_groupã®å­¦ç¿’ç‡ã‚’å‡ºåŠ›
    optimizer.step()	# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’æ›´æ–°
    scheduler.step()	# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’æ›´æ–°
```

## ğŸ“š å‚è€ƒãƒªãƒ³ã‚¯

- [PyTorch CosineAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)
