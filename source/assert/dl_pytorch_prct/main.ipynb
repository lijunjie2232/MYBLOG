{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc80bcbe",
   "metadata": {},
   "source": [
    "## prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08eb258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ubuntu/.cache/kagglehub/datasets/aadityasinghal/facial-expression-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "data_root = kagglehub.dataset_download(\"aadityasinghal/facial-expression-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c75a914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/.cache/kagglehub/datasets/aadityasinghal/facial-expression-dataset/versions/1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_root = Path(data_root)\n",
    "data_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8164ac6",
   "metadata": {},
   "source": [
    "## build data transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32424a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transformer = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),  # 224x224\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "val_transformer = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d21142",
   "metadata": {},
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7708252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_dataset = ImageFolder(\n",
    "    root=data_root / \"train\" / \"train\", transform=train_transformer\n",
    ")\n",
    "val_dataset = ImageFolder(root=data_root / \"test\" / \"test\", transform=val_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e2eee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 28709\n",
       "    Root location: /home/ubuntu/.cache/kagglehub/datasets/aadityasinghal/facial-expression-dataset/versions/1/train/train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               RandomCrop(size=(224, 224), padding=None)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
       "               RandomVerticalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0842ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bb3b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape, train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f960ac09",
   "metadata": {},
   "source": [
    "## build dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b48313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73566d",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ec710",
   "metadata": {},
   "source": [
    "### layers code copies from ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "934e9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeebb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = (\n",
    "            d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]\n",
    "        )  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8a7b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard convolution module with batch normalization and activation.\n",
    "\n",
    "    Attributes:\n",
    "        conv (nn.Conv2d): Convolutional layer.\n",
    "        bn (nn.BatchNorm2d): Batch normalization layer.\n",
    "        act (nn.Module): Activation function layer.\n",
    "        default_act (nn.Module): Default activation function (SiLU).\n",
    "    \"\"\"\n",
    "\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"\n",
    "        Initialize Conv layer with given parameters.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Number of input channels.\n",
    "            c2 (int): Number of output channels.\n",
    "            k (int): Kernel size.\n",
    "            s (int): Stride.\n",
    "            p (int, optional): Padding.\n",
    "            g (int): Groups.\n",
    "            d (int): Dilation.\n",
    "            act (bool | nn.Module): Activation function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = (\n",
    "            self.default_act\n",
    "            if act is True\n",
    "            else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply convolution, batch normalization and activation to input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"\n",
    "        Apply convolution and activation without batch normalization.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        return self.act(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c2980ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
    "        \"\"\"\n",
    "        Initialize a standard bottleneck module.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            shortcut (bool): Whether to use shortcut connection.\n",
    "            g (int): Groups for convolutions.\n",
    "            k (Tuple[int, int]): Kernel sizes for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, k[0], 1)\n",
    "        self.cv2 = Conv(c_, c2, k[1], 1, g=g)\n",
    "        self.add = shortcut and c1 == c2\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\n",
    "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2275eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2f(nn.Module):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):\n",
    "        \"\"\"\n",
    "        Initialize a CSP bottleneck with 2 convolutions.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            n (int): Number of Bottleneck blocks.\n",
    "            shortcut (bool): Whether to use shortcut connections.\n",
    "            g (int): Groups for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, 2 * self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.ModuleList(\n",
    "            Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0)\n",
    "            for _ in range(n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through C2f layer.\"\"\"\n",
    "        y = list(self.cv1(x).chunk(2, 1))\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "    def forward_split(self, x):\n",
    "        \"\"\"Forward pass using split() instead of chunk().\"\"\"\n",
    "        y = self.cv1(x).split((self.c, self.c), 1)\n",
    "        y = [y[0], y[1]]\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b34abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C3(nn.Module):\n",
    "    \"\"\"CSP Bottleneck with 3 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the CSP Bottleneck with 3 convolutions.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            n (int): Number of Bottleneck blocks.\n",
    "            shortcut (bool): Whether to use shortcut connections.\n",
    "            g (int): Groups for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv(c1, c_, 1, 1)\n",
    "        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.Sequential(\n",
    "            *(\n",
    "                Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0)\n",
    "                for _ in range(n)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the CSP bottleneck with 3 convolutions.\"\"\"\n",
    "        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7533a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C3k(C3):\n",
    "    \"\"\"C3k is a CSP bottleneck module with customizable kernel sizes for feature extraction in neural networks.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):\n",
    "        \"\"\"\n",
    "        Initialize C3k module.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            n (int): Number of Bottleneck blocks.\n",
    "            shortcut (bool): Whether to use shortcut connections.\n",
    "            g (int): Groups for convolutions.\n",
    "            e (float): Expansion ratio.\n",
    "            k (int): Kernel size.\n",
    "        \"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        # self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))\n",
    "        self.m = nn.Sequential(\n",
    "            *(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c91d9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C3k2(C2f):\n",
    "    \"\"\"Faster Implementation of CSP Bottleneck with 2 convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):\n",
    "        \"\"\"\n",
    "        Initialize C3k2 module.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channels.\n",
    "            c2 (int): Output channels.\n",
    "            n (int): Number of blocks.\n",
    "            c3k (bool): Whether to use C3k blocks.\n",
    "            e (float): Expansion ratio.\n",
    "            g (int): Groups for convolutions.\n",
    "            shortcut (bool): Whether to use shortcut connections.\n",
    "        \"\"\"\n",
    "        super().__init__(c1, c2, n, shortcut, g, e)\n",
    "        self.m = nn.ModuleList(\n",
    "            (\n",
    "                C3k(self.c, self.c, 2, shortcut, g)\n",
    "                if c3k\n",
    "                else Bottleneck(self.c, self.c, shortcut, g)\n",
    "            )\n",
    "            for _ in range(n)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42dbf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Area-attention module for YOLO models, providing efficient attention mechanisms.\n",
    "\n",
    "    This module implements an area-based attention mechanism that processes input features in a spatially-aware manner,\n",
    "    making it particularly effective for object detection tasks.\n",
    "\n",
    "    Attributes:\n",
    "        area (int): Number of areas the feature map is divided.\n",
    "        num_heads (int): Number of heads into which the attention mechanism is divided.\n",
    "        head_dim (int): Dimension of each attention head.\n",
    "        qkv (Conv): Convolution layer for computing query, key and value tensors.\n",
    "        proj (Conv): Projection convolution layer.\n",
    "        pe (Conv): Position encoding convolution layer.\n",
    "\n",
    "    Methods:\n",
    "        forward: Applies area-attention to input tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> attn = AAttn(dim=256, num_heads=8, area=4)\n",
    "        >>> x = torch.randn(1, 256, 32, 32)\n",
    "        >>> output = attn(x)\n",
    "        >>> print(output.shape)\n",
    "        torch.Size([1, 256, 32, 32])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, area=1):\n",
    "        \"\"\"\n",
    "        Initialize an Area-attention module for YOLO models.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Number of hidden channels.\n",
    "            num_heads (int): Number of heads into which the attention mechanism is divided.\n",
    "            area (int): Number of areas the feature map is divided, default is 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.area = area\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim = dim // num_heads\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "\n",
    "        self.qkv = Conv(dim, all_head_dim * 3, 1, act=False)\n",
    "        self.proj = Conv(all_head_dim, dim, 1, act=False)\n",
    "        self.pe = Conv(all_head_dim, dim, 7, 1, 3, g=dim, act=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process the input tensor through the area-attention.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor after area-attention.\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        N = H * W\n",
    "\n",
    "        qkv = self.qkv(x).flatten(2).transpose(1, 2)\n",
    "        if self.area > 1:\n",
    "            qkv = qkv.reshape(B * self.area, N // self.area, C * 3)\n",
    "            B, N, _ = qkv.shape\n",
    "        q, k, v = (\n",
    "            qkv.view(B, N, self.num_heads, self.head_dim * 3)\n",
    "            .permute(0, 2, 3, 1)\n",
    "            .split([self.head_dim, self.head_dim, self.head_dim], dim=2)\n",
    "        )\n",
    "        attn = (q.transpose(-2, -1) @ k) * (self.head_dim**-0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = v @ attn.transpose(-2, -1)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        v = v.permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.area > 1:\n",
    "            x = x.reshape(B // self.area, N * self.area, C)\n",
    "            v = v.reshape(B // self.area, N * self.area, C)\n",
    "            B, N, _ = x.shape\n",
    "\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n",
    "        v = v.reshape(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        x = x + self.pe(v)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86b48755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Area-attention block module for efficient feature extraction in YOLO models.\n",
    "\n",
    "    This module implements an area-attention mechanism combined with a feed-forward network for processing feature maps.\n",
    "    It uses a novel area-based attention approach that is more efficient than traditional self-attention while\n",
    "    maintaining effectiveness.\n",
    "\n",
    "    Attributes:\n",
    "        attn (AAttn): Area-attention module for processing spatial features.\n",
    "        mlp (nn.Sequential): Multi-layer perceptron for feature transformation.\n",
    "\n",
    "    Methods:\n",
    "        _init_weights: Initializes module weights using truncated normal distribution.\n",
    "        forward: Applies area-attention and feed-forward processing to input tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> block = ABlock(dim=256, num_heads=8, mlp_ratio=1.2, area=1)\n",
    "        >>> x = torch.randn(1, 256, 32, 32)\n",
    "        >>> output = block(x)\n",
    "        >>> print(output.shape)\n",
    "        torch.Size([1, 256, 32, 32])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=1.2, area=1):\n",
    "        \"\"\"\n",
    "        Initialize an Area-attention block module.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of heads into which the attention mechanism is divided.\n",
    "            mlp_ratio (float): Expansion ratio for MLP hidden dimension.\n",
    "            area (int): Number of areas the feature map is divided.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = AAttn(dim, num_heads=num_heads, area=area)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            Conv(dim, mlp_hidden_dim, 1), Conv(mlp_hidden_dim, dim, 1, act=False)\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"\n",
    "        Initialize weights using a truncated normal distribution.\n",
    "\n",
    "        Args:\n",
    "            m (nn.Module): Module to initialize.\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through ABlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor after area-attention and feed-forward processing.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(x)\n",
    "        return x + self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9306a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C2f(nn.Module):\n",
    "    \"\"\"\n",
    "    Area-Attention C2f module for enhanced feature extraction with area-based attention mechanisms.\n",
    "\n",
    "    This module extends the C2f architecture by incorporating area-attention and ABlock layers for improved feature\n",
    "    processing. It supports both area-attention and standard convolution modes.\n",
    "\n",
    "    Attributes:\n",
    "        cv1 (Conv): Initial 1x1 convolution layer that reduces input channels to hidden channels.\n",
    "        cv2 (Conv): Final 1x1 convolution layer that processes concatenated features.\n",
    "        gamma (nn.Parameter | None): Learnable parameter for residual scaling when using area attention.\n",
    "        m (nn.ModuleList): List of either ABlock or C3k modules for feature processing.\n",
    "\n",
    "    Methods:\n",
    "        forward: Processes input through area-attention or standard convolution pathway.\n",
    "\n",
    "    Examples:\n",
    "        >>> m = A2C2f(512, 512, n=1, a2=True, area=1)\n",
    "        >>> x = torch.randn(1, 512, 32, 32)\n",
    "        >>> output = m(x)\n",
    "        >>> print(output.shape)\n",
    "        torch.Size([1, 512, 32, 32])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        c1,\n",
    "        c2,\n",
    "        n=1,\n",
    "        a2=True,\n",
    "        area=1,\n",
    "        residual=False,\n",
    "        mlp_ratio=2.0,\n",
    "        e=0.5,\n",
    "        g=1,\n",
    "        shortcut=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Area-Attention C2f module.\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Number of input channels.\n",
    "            c2 (int): Number of output channels.\n",
    "            n (int): Number of ABlock or C3k modules to stack.\n",
    "            a2 (bool): Whether to use area attention blocks. If False, uses C3k blocks instead.\n",
    "            area (int): Number of areas the feature map is divided.\n",
    "            residual (bool): Whether to use residual connections with learnable gamma parameter.\n",
    "            mlp_ratio (float): Expansion ratio for MLP hidden dimension.\n",
    "            e (float): Channel expansion ratio for hidden channels.\n",
    "            g (int): Number of groups for grouped convolutions.\n",
    "            shortcut (bool): Whether to use shortcut connections in C3k blocks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_ = int(c2 * e)  # hidden channels\n",
    "        assert c_ % 32 == 0, \"Dimension of ABlock be a multiple of 32.\"\n",
    "\n",
    "        self.cv1 = Conv(c1, c_, 1, 1)\n",
    "        self.cv2 = Conv((1 + n) * c_, c2, 1)\n",
    "\n",
    "        self.gamma = (\n",
    "            nn.Parameter(0.01 * torch.ones(c2), requires_grad=True)\n",
    "            if a2 and residual\n",
    "            else None\n",
    "        )\n",
    "        self.m = nn.ModuleList(\n",
    "            (\n",
    "                nn.Sequential(\n",
    "                    *(ABlock(c_, c_ // 32, mlp_ratio, area) for _ in range(2))\n",
    "                )\n",
    "                if a2\n",
    "                else C3k(c_, c_, 2, shortcut, g)\n",
    "            )\n",
    "            for _ in range(n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through A2C2f layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor after processing.\n",
    "        \"\"\"\n",
    "        y = [self.cv1(x)]\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        y = self.cv2(torch.cat(y, 1))\n",
    "        if self.gamma is not None:\n",
    "            return x + self.gamma.view(-1, len(self.gamma), 1, 1) * y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29e60fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify(nn.Module):\n",
    "    \"\"\"YOLO classification head, i.e. x(b,c1,20,20) to x(b,c2).\"\"\"\n",
    "\n",
    "    export = False  # export mode\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):\n",
    "        \"\"\"Initializes YOLO classification head to transform input tensor from (b,c1,20,20) to (b,c2) shape.\"\"\"\n",
    "        super().__init__()\n",
    "        c_ = 1280  # efficientnet_b0 size\n",
    "        self.conv = Conv(c1, c_, k, s, p, g)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # to x(b,c_,1,1)\n",
    "        self.drop = nn.Dropout(p=0.0, inplace=True)\n",
    "        self.linear = nn.Linear(c_, c2)  # to x(b,c2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Performs a forward pass of the YOLO model on input image data.\"\"\"\n",
    "        if isinstance(x, list):\n",
    "            x = torch.cat(x, 1)\n",
    "        x = self.linear(self.drop(self.pool(self.conv(x)).flatten(1)))\n",
    "        if self.training:\n",
    "            return x\n",
    "        y = x.softmax(1)  # get final output\n",
    "        return y if self.export else (y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53eca9",
   "metadata": {},
   "source": [
    "### model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionNet(nn.Module):\n",
    "    def __init__(self, c1: int = 3, nc: int = 7):\n",
    "        \"\"\"\n",
    "        Initialize classification model\n",
    "\n",
    "        Args:\n",
    "            c1 (int): Input channel size\n",
    "            nc (int): Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate scaling parameters from config\n",
    "\n",
    "        # Build backbone\n",
    "        self.backbone = nn.ModuleList(\n",
    "            [\n",
    "                # 0-P1/2\n",
    "                Conv(c1=3, c2=16, k=3, s=2),\n",
    "                # 1-P2/4\n",
    "                Conv(c1=16, c2=32, k=3, s=2),\n",
    "                # 2-C3k2 block\n",
    "                C3k2(c1=32, c2=64, n=1, e=0.25),\n",
    "                # 3-P3/8\n",
    "                Conv(c1=64, c2=128, k=3, s=2),\n",
    "                # 4-C3k2 block\n",
    "                C3k2(c1=128, c2=128, n=1, e=0.25),\n",
    "                # 5-P4/16\n",
    "                Conv(c1=128, c2=128, k=3, s=2),\n",
    "                # 6-A2C2f block\n",
    "                A2C2f(c1=128, c2=128, n=2, a2=True, area=4, e=0.5),\n",
    "                # 7-P5/32\n",
    "                Conv(c1=128, c2=256, k=3, s=2),\n",
    "                # 8-A2C2f block\n",
    "                A2C2f(c1=256, c2=256, n=2, a2=True, area=1, e=0.5),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Build classification head\n",
    "        self.classify = Classify(256, nc)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Pass through backbone\n",
    "        for layer in self.backbone:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Pass through classification head\n",
    "        x = self.classify(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3b6fa58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d2b91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MotionNet(\n",
    "    nc=len(train_dataset.classes),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a84fe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = i[0]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "61ac08df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Conv(\n",
      "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): SiLU()\n",
      ")\n",
      "1\n",
      "Conv(\n",
      "  (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): SiLU()\n",
      ")\n",
      "2\n",
      "C3k2(\n",
      "  (cv1): Conv(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): SiLU()\n",
      "  )\n",
      "  (cv2): Conv(\n",
      "    (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): SiLU()\n",
      "  )\n",
      "  (m): ModuleList(\n",
      "    (0): Bottleneck(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU()\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Conv(\n",
      "  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): SiLU()\n",
      ")\n",
      "4\n",
      "C3k2(\n",
      "  (cv1): Conv(\n",
      "    (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): SiLU()\n",
      "  )\n",
      "  (cv2): Conv(\n",
      "    (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): SiLU()\n",
      "  )\n",
      "  (m): ModuleList(\n",
      "    (0): Bottleneck(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU()\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "5\n",
      "Conv(\n",
      "  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): SiLU()\n",
      ")\n",
      "6\n",
      "A2C2f(\n",
      "  (cv1): Conv(\n",
      "    (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): SiLU()\n",
      "  )\n",
      "  (cv2): Conv(\n",
      "    (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): SiLU()\n",
      "  )\n",
      "  (m): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): ABlock(\n",
      "        (attn): AAttn(\n",
      "          (qkv): Conv(\n",
      "            (conv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (proj): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (pe): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "        )\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ABlock(\n",
      "        (attn): AAttn(\n",
      "          (qkv): Conv(\n",
      "            (conv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (proj): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (pe): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64, bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "        )\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "7\n",
      "Conv(\n",
      "  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): SiLU()\n",
      ")\n",
      "8\n",
      "A2C2f(\n",
      "  (cv1): Conv(\n",
      "    (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): SiLU()\n",
      "  )\n",
      "  (cv2): Conv(\n",
      "    (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): SiLU()\n",
      "  )\n",
      "  (m): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): ABlock(\n",
      "        (attn): AAttn(\n",
      "          (qkv): Conv(\n",
      "            (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (proj): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (pe): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "        )\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ABlock(\n",
      "        (attn): AAttn(\n",
      "          (qkv): Conv(\n",
      "            (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (proj): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (pe): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128, bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "        )\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU()\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1801,  0.0032, -0.0281,  0.0566, -0.1107,  0.0877, -0.0022]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model(data.to(device))\n",
    "model(torch.randn(1, 3, 224, 224).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f58d8",
   "metadata": {},
   "source": [
    "## build loss, optimizer and lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "115e85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1,)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06cd059",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d836b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7cfe099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f81a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, desc=\"train\", leave=False)\n",
    "    total_loss = torch.tensor(0)\n",
    "    total_count = 0\n",
    "    ateru = 0\n",
    "    for batch_idx, (data, target) in enumerate(loop):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        tota_count += len(data)\n",
    "        # calc acc\n",
    "        ateru += (output.argmax(dim=1) == target).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(\n",
    "            {\n",
    "                \"ACC\": ateru / total_count,\n",
    "                \"LOSS\": total_loss / (batch_idx + 1),\n",
    "            }\n",
    "        )\n",
    "    scheduler.step()\n",
    "    return ateru / total_count, total_loss / (batch_idx + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ae36d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    loop = tqdm(val_loader, desc=\"val\", leave=False)\n",
    "    val_loss = 0.0\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    for i, (inputs, labels) in enumerate(loop):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item() * inputs.size(0)\n",
    "        total_num += labels.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        loop.set_postfix(loss=val_loss / (i + 1), acc=total_correct / total_num)\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    return total_correct / total_num, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78e5040f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f318ba86a22e414a83b93474c587faf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a1722e593f4ee4bb3d58e336851b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1795 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 224, 3, 3], expected input[16, 3, 224, 224] to have 224 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[32m      3\u001b[39m     loop.set_description(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_acc, train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     loop.set_postfix(\n\u001b[32m      6\u001b[39m         train_acc=train_acc,\n\u001b[32m      7\u001b[39m         train_loss=train_loss,\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m     val_acc, val_loss = test_epoch(model, val_dataloader, criterion)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, criterion)\u001b[39m\n\u001b[32m      9\u001b[39m target = target.to(device)\n\u001b[32m     10\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss = criterion(output, target)\n\u001b[32m     13\u001b[39m tota_count += \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/anaconda3/envs/dl_pytorch_prct/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/anaconda3/envs/dl_pytorch_prct/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mMotionNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Pass through backbone\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backbone:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Pass through classification head\u001b[39;00m\n\u001b[32m     48\u001b[39m x = \u001b[38;5;28mself\u001b[39m.classify(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/anaconda3/envs/dl_pytorch_prct/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/anaconda3/envs/dl_pytorch_prct/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     40\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    Apply convolution, batch normalization and activation to input tensor.\u001b[39;00m\n\u001b[32m     42\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28mself\u001b[39m.bn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/anaconda3/envs/dl_pytorch_prct/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/anaconda3/envs/dl_pytorch_prct/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/anaconda3/envs/dl_pytorch_prct/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/anaconda3/envs/dl_pytorch_prct/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [16, 224, 3, 3], expected input[16, 3, 224, 224] to have 224 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "loop = tqdm(range(epochs))\n",
    "for epoch in loop:\n",
    "    loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    train_acc, train_loss = train_epoch(model, train_dataloader, optimizer, criterion)\n",
    "    loop.set_postfix(\n",
    "        train_acc=train_acc,\n",
    "        train_loss=train_loss,\n",
    "    )\n",
    "    val_acc, val_loss = test_epoch(model, val_dataloader, criterion)\n",
    "    loop.set_postfix(\n",
    "        train_acc=train_acc,\n",
    "        train_loss=train_loss,\n",
    "        val_acc=val_acc,\n",
    "        val_loss=val_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598479d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd53c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58965c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab47f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8931a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba88690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_pytorch_prct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
